{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# we will also remove punctuations using regex\n",
    "# along with removing stop words (in the same loop)\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class nlp_pipe:\n",
    "    \n",
    "    def __init__(self, vectorizer = CountVectorizer(), cleaning_function = None, tokenizer = None, stemmer= None):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "    \n",
    "    def cleaning_function(self, text, tokenizer, stemmer):\n",
    "        #make all chars lowercase\n",
    "        # simple list comprehension to exclude stop words   \n",
    "     #   text = [word for word in text if word not in stopwords.words('english')]\n",
    "        #clean_text = []\n",
    "        #for i in range(len(text)):\n",
    "         #   r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "          #  r = r.lower()\n",
    "           # clean_text.append(r)                                      \n",
    "        #return clean_text\n",
    "        cleaned_text = [] #main list\n",
    "        for post in text: #go through each word in the list\n",
    "            cleaned_words = [] #create a new list\n",
    "            for word in tokenizer(post): #go through each tokenized word\n",
    "                low_word = word.lower() #make word lowercase\n",
    "                if stemmer: #check stems of the words\n",
    "                    low_word = stemmer.stem(low_word) #reduce the stems of the words to the root as in playing becomes play\n",
    "                cleaned_words.append(low_word) #append the clean words into the new list\n",
    "            cleaned_text.append(' '.join(cleaned_words)) #append to the main list\n",
    "        return cleaned_text #return the cleaned text\n",
    "        \n",
    "        \n",
    "    #def tokenizer(self, text):\n",
    "        #sentence = nltk.sent_tokenize(text)\n",
    "        # word_tokenize function\n",
    "        #words = nltk.word_tokenize(text)\n",
    "        \n",
    "    def fit(self, text):\n",
    "        ct = self.cleaning_function(text,self.tokenizer, self.stemmer)\n",
    "        #ct = self.cleaning_function(self, text)\n",
    "        self.vectorizer.fit(ct)  \n",
    "      \n",
    "    \n",
    "    def transform(self, text):\n",
    "       #transform the text into an array\n",
    "        ct = self.cleaning_function(text,self.tokenizer, self.stemmer)\n",
    "        return  self.vectorizer.transform(ct) \n",
    "             # return ' '.join(words_processed)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 4, 1, 1, 3, 0, 1, 2, 0, 0, 1, 1, 1, 2, 1, 0, 8, 1,\n",
       "        1, 2, 0, 3, 2, 1, 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So what I want is the ability to do something like:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "corpus = [\"Normalization is a technique\", \"often applied as part of data preparation\",\" before training machine learning models.\",\" The goal of normalization is to rescale the values of numeric features in the dataset without distorting differences in the ranges of values or losing information. This can be achieved in PyCaret using the normalize parameter within the setup. There are several methods available for normalization, by default it uses ‘zscore’ to normalize the data, which can be changed using the normalize_method parameter within setup.\"]\n",
    "\n",
    "#in this one I referred the cleaning function to the one inside the class and you forgot to add .tokenize after TreebankWordTokenizer()\n",
    "nlp = nlp_pipe(vectorizer = CountVectorizer(), cleaning_function = cleaning_function, tokenizer = TreebankWordTokenizer().tokenize, stemmer= PorterStemmer())\n",
    "nlp.fit(corpus)\n",
    "nlp.transform(corpus).toarray() #I added .toarray()\n",
    "#Which should return the test corpus in its vectorizer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
